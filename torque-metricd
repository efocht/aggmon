#!/usr/bin/python2
##########################################################################
#                                                                        #
#               Copyright (C) 2013 - 2014 NEC HPC Europe.                #
#                                                                        #
#  These coded instructions, statements, and computer programs  contain  #
#  unpublished  proprietary  information of NEC HPC Europe, and are      #
#  are protected by Federal copyright law.  They  may  not be disclosed  #
#  to  third  parties  or copied or duplicated in any form, in whole or  #
#  in part, without the prior written consent of NEC HPC Europe.         #
#                                                                        #
##########################################################################
#
import ConfigParser
import glob
import md5
import os
import signal
import subprocess
import socket
import sys
import logging
import logging.handlers
import time
import psutil
import copy
from metric_store import MongoDBMetricStore, MongoDBJobList, JMetric

"""
This script mainly parses the PBS Torque accounting events and puts these
events as metrics into a database (MongoDB).
Past events are retrieved from PBS Torque log files (backlog). After these
files are read the script behaves similar to a "tail -f" command and watches
for new events in the current log file.
In addition to storing metrics in a database there are some other tasks (JobTask)
that are executed on job start and stop events.
"""

class JobTask:
    """ Interface definition for tasks to be executed on job start and stop events """
    def addJob( self, metric ):
        """ This method is called if an job "started" or "restarted" event is received """
        raise NotImplementedException()

    def removeJob( self, name ):
        """ This method is called if an job "aborted", "checkpointed", "deleted" and "exited" event is received """
        raise NotImplementedException()

class JobCache(JobTask):
    """ Temporary storage (memory) for Jobs with unique JobID (name) """
    def __init__( self ):
        self.__cache = {}

    def addJob( self, metric ):
        self.__cache[metric["name"]] = copy.deepcopy( metric )

    def removeJob( self, name ):
        try:
            del self.__cache[name]
        except:
            pass

    def getList( self ):
        return self.__cache.values()

class JobList(JobTask):
    """ Permanent storage (MongoDB) for Jobs with unique JobID (name) """
    def __init__( self, conf ):
        self.__create_joblist = conf["create_joblist"]
        if self.__create_joblist:
            self.__joblist = MongoDBJobList( host_name=conf["mongodb_host"], username=conf["mongodb_uname"], password=conf["mongodb_pw"] )

    def addJob( self, metric ):
        if self.__create_joblist:
            self.__joblist.addJob( metric )

    def removeJob( self, name ):
        if self.__create_joblist:
            self.__joblist.removeJob( name )

    def getList( self ):
        return self.__joblist.getList()

class Tagger(JobTask):
    """ Execute an external program for currently running jobs that e.g. adds Job information to metrics"""
    def __init__(self, conf ):
        self.__call_tagger = conf["call_tagger"]
        if not self.__call_tagger:
            return
        self.__cmd = conf["tagger_cmd"]
        
    def addJob( self, metric ):
        if not self.__call_tagger:
            return
        nodelist = []
        if isinstance( metric["cnodes"], dict ):
            nodelist = metric["cnodes"].keys()
        elif isinstance( metric["cnodes"], list ):
            nodelist = metric["cnodes"]
        if len(nodelist) == 1:
            regex_str = nodelist[0]
        else:
            regex_str = "^(" + "|".join( nodelist ) + ")$"
        csv_str = ",".join( nodelist )
        self.__invoke(self.__cmd, ACTION="add", CNODES_REGEX=regex_str, CNODES_CSV=csv_str, **metric )

    def removeJob( self, name ):
        if not self.__call_tagger:
            return
        self.__invoke(self.__cmd, ACTION="del", **metric )

    @staticmethod
    def __invoke( cmd, **kwargs ):
        env = os.environ.copy()
        for k, v in kwargs.items():
            env["JOB_" + k.upper()] = str( v )
        try:
            args = ["/bin/bash", "-c", cmd]
            r = subprocess.call( args, env=env )
        except Exception as e:
            log.debug( "exception while calling command: %s" % str( e ) )
        if r != 0:
            log.warn( "calling command (%s) failed!" % cmd )


# Constants
TORQUE_PROC = "pbs_server"
TORQUE_ACCOUNTING_PATH = "/var/spool/torque/server_priv/accounting"
TORQUE_LOG_MARKER = {"A" : "aborted",         # job has been aborted by server
                     "C" : "checkpointed",    # job has been checkpointed and stopped
                     "D" : "deleted",         # job has been deleted
                     "E" : "exited",          # job has exited (either successfully or unsuccessfully)
                     "Q" : "queued",          # job has been submitted/queued
                     "R" : "rerun",           # an attempt to rerun the job has been made
                     "S" : "started",         # an attempt to start the job has been made
                     "T" : "restarted",       # an attempt to restart the job has been made (after checkpointed)
                    }
LOG_LEVELS = {"error"   : logging.ERROR,
              "warning" : logging.WARNING,
              "info"    : logging.INFO,
              "debug"   : logging.DEBUG,
             }
CONF_FILE = "/etc/torque-metricd.conf"

global tag_file
global tag_new
global acc_log

CONFIG_DEFAULTS = {"acc_file_dir"   : TORQUE_ACCOUNTING_PATH,
                   "tag_file"       : TORQUE_ACCOUNTING_PATH + "/tag",
                   "max_acc_files"  : 7,
                   "log_level"      : "warning",
                   "to_syslog"      : "True",
                   "mongodb_host"   : "localhost",
                   "mongodb_dbname" : "metric",
                   "mongodb_uname"  : "",
                   "mongodb_pw"     : "",
                   "include_cpus"   : "False",
                   "call_tagger"    : "False",
                   "create_joblist" : "True",
                  }


def sig_handler( sig, frame ):
    if tag_new:
        put_tag( tag_file, tag_new, acc_log )
    log.info( "terminating due to signal %d..." % sig)
    sys.exit( 0 )

def md5sum( s ):
    m = md5.new()
    m.update( s )
    return m.hexdigest()

def pidof( name ):
    try:
        return [p.pid for p in psutil.process_iter() if p.name() == name]
    except:
        # due to changes in psutil
        return [p.pid for p in psutil.process_iter() if p.name == name]

def torque_pid():
    while True:
        pids = pidof( TORQUE_PROC )
        if pids:
            return pids[0]
        log.info( "waiting for Torque (%s) to start..." % TORQUE_PROC )
        time.sleep( 60 )

def is_file_open( pid, file_name ):
    dir = os.path.join( "/proc", str( pid ), "fd" )
    if not os.path.exists( dir ):
        log.warn( "%s seems to have terminated!" % TORQUE_PROC )
        return False
    if not os.access( dir, os.R_OK | os.X_OK ):
        log.error( "no permissions to access: %s! Terminating..." % dir )
        sys.exit( 1 )
    for fd in os.listdir( dir ):
        full_name = os.path.join( dir, fd )
        try:
            file = os.readlink( full_name )
            if file == file_name:
                return True
        except:
            pass
    return False

def tail( pid, file_name ):
    fd = open( file_name )
    fd.seek( 0, 2 )
    sleep = 0.001
    buf = ""
    eof = False
    while is_file_open( pid, file_name ):
        while not eof:
            # while not at EOF (indicated by readline returning "")
            time.sleep( sleep )
            buf = buf + fd.readline()
            if buf == "":
                eof = True
                if sleep < 1.0:
                    sleep += 0.001
                continue
            sleep = 0.001
            # complete Torque lines always end with "\n"
            if buf[-1] != "\n":#
                eof = False
                continue
            line = buf
            buf = ""
            yield line
        eof = False
    fd.close()

def parse_torque_accounting_log( line ):
    import time
    line = line.strip()
    if not line:
        log.warn( "ignoring empty line" )
        return None
    try:
        # example header: 08/08/2014 11:32:23;S;50438;<payload>
        comp = line.split( ";", 3 ) 
        assert comp[1] in TORQUE_LOG_MARKER.keys(), "unknown event: '%s'" % comp[1]
        timestamp = int( time.mktime( time.strptime( comp[0], "%m/%d/%Y %H:%M:%S" ) ) )     # log (not job) timestamp (time, T)
        value = TORQUE_LOG_MARKER[comp[1]]                                                  # event type (value, V)
        name = comp[2].replace( ".", "_" )                                                  # jobid (name, N)
        payload = comp[3]                                                                   # attribute set
        host_name = socket.gethostname()                                                    # add local hostname
        metric = JMetric( name=name, source="Torque", time=timestamp, host=host_name, value=value )
    except Exception as e:
        log.warn( "parsing header (%s) failed: %s" % (str( e ), line[0:30]) )
        return None
    if not payload:
        return metric
    for attr_str in payload.split( " " ):
        # example payload: user=myuname group=sthaber jobname=STDIN queue=workq ctime=1407490338 qtime=1407490338 etime=1407490338 start=1407490343
        #     owner=myuname@sb-master exec_host=<exec_host> Resource_List.ncpus=1 Resource_List.neednodes=1:ppn=1:foo:ghz-2.6:mhz-2601:ddr1866:qlogic
        #     Resource_List.nodect=1 Resource_List.nodes=1:ppn=1:foo:ghz-2.6:mhz-2601:ddr1866:qlogic
        attr_str = attr_str.replace( ".", "_" )
        try:
            k, v = attr_str.split( "=", 1 )
        except Exception as e:
            log.warn( "event (%s...), failed to parse attributes: '%s'" % (line[0:30], attr_str) )
            return None
        if k in ["ctime", "qtime", "etime", "start", "end"]:
            #ctime: job creation time
            #qtime: job was queued
            #etime: job became eligible to run
            #start: job start time
            #end: job ended
            v = int( v )
        elif k in ["user", "group", "jobname", "queue", "owner"]:
            pass
            # any special treatment here?
        elif k == "exec_host":
            # example exec_host: sabi36/19+sabi36/18+sabi36/17+sabi36/16+sabi36/15+sabi36/14+sabi36/13+sabi36/12+sabi36/11...
            nodes = sorted( list( set( [r.split( "/" )[0] for r in v.split( "+" )] ) ) )
            k = "cnodes"
            if conf["include_cpus"]:
                cpus = {}
                for n in nodes:
                    cpus[n] = sorted( list( set( [int( r.split( "/", 2 )[1] ) for r in v.split( "+" ) if r.split( "/", 2 )[0] == n] ) ) )
                v = cpus
            else:
                v = nodes
        elif k.startswith( "Resource_List" ): 
            k = k.replace( "Resource_List", "RL" )
        metric[k] = v
    return metric

def get_tag( tag_file ):
    tag = None
    log_file_name = None
    if os.path.isfile( tag_file ):
        try:
            tag, log_file_name = open( tag_file ).read().split( " ", 2 )
        except Exception as e:
            log.debug( "failed to read tag file (%s): %s" % (tag_file, str( e )) )
        if tag and log_file_name:
            log.debug( "tag file (%s) found: %s %s" % (tag_file, tag, log_file_name) )
        else:
            tag = None
            log_file_name = None
    return tag, log_file_name

def put_tag( tag_file, tag, log_file_name ):
    open( tag_file, "w" ).write( tag + " " + log_file_name )
    log.debug( "wrote tag (%s): %s %s" % (tag_file, tag, log_file_name) )

def update_job_tasks( metric , objs ):
    if metric["value"] in ["started", "restarted"]:
        # "rerun" is not considered here since it is followed by a "started" event
        for obj in objs:
            try:
                obj.addJob( metric )
            except Exception as e:
                log.error( "Failed to add job metric to %s: %s" % ( obj.__class__.__name__, str( e )) )
    elif metric["value"] in ["aborted", "checkpointed", "deleted", "exited"]:
        for obj in objs:
            try:
                obj.removeJob( metric["name"] )
            except Exception as e:
                log.error( "Failed to remove job metric of %s: %s" % ( obj.__class__.__name__, str( e )) )
    else:
        return    

def read_config():
    conf = {}
    config = ConfigParser.RawConfigParser( CONFIG_DEFAULTS )
    config.add_section( "daemon" )
    config.read( CONF_FILE )
    conf["log_level"] = config.get( "daemon", "log_level" )
    conf["to_syslog"] = config.getboolean( "daemon", "to_syslog" )
    conf["acc_file_dir"] = config.get( "daemon", "acc_file_dir" )
    conf["tag_file"] = config.get( "daemon", "tag_file" )
    conf["max_acc_files"] = config.getint( "daemon", "max_acc_files" )
    conf["mongodb_host"] = config.get( "daemon", "mongodb_host" )
    conf["mongodb_dbname"] = config.get( "daemon", "mongodb_dbname" )
    conf["mongodb_uname"] = config.get( "daemon", "mongodb_uname" )
    conf["mongodb_pw"] = config.get( "daemon", "mongodb_pw" )
    conf["include_cpus"] = config.getboolean( "daemon", "include_cpus" )
    conf["create_joblist"] = config.getboolean( "daemon", "create_joblist" )
    conf["call_tagger"] = config.getboolean( "daemon", "call_tagger" )
    if conf["call_tagger"]:
        conf["tagger_cmd"] = config.get("daemon", "tagger_cmd" )
    return conf

def connect_metric_store( conf ):
    try:
        store = MongoDBMetricStore( host_name=conf["mongodb_host"], username=conf["mongodb_uname"], password=conf["mongodb_pw"], db_name=conf["mongodb_dbname"] )
    except Exception as e:
        log.error( "failed to connect to MongoDB (%s)! Terminating..." % str( e ) )
        sys.exit( 1 )
    return store

# main
conf = read_config()

# set logging up
try:
    log = logging.getLogger( __name__ )
    log_level = LOG_LEVELS[conf["log_level"].lower()]
except:
    log_level = config_defaults["log_level"]
    log.warn( "unknown log level '%s'! Defaults to '%s'." % (log_level, config_defaults["log_level"]) )
log.setLevel( log_level )
if conf["to_syslog"]:
    handler = logging.handlers.SysLogHandler( address="/dev/log" )
else:
    handler = logging.StreamHandler()
log.addHandler( handler )
for k, v in conf.items():
    log.debug( "conf: %s = %s" % (k, str( v )) )
tag_file = conf["tag_file"]

# open database connection
store = connect_metric_store( conf )

# set signal handling up
tag_new = None
signal.signal(signal.SIGTERM, sig_handler)
signal.signal(signal.SIGINT, sig_handler)

# import backlog, add all job events to metric store, create cache of currently running jobs
cache = JobCache()
tag, last_log = get_tag( tag_file )
tag_found = False
file_names = sorted( glob.glob( conf["acc_file_dir"] + "/[0-9]*" ) )
start = len( file_names ) - conf["max_acc_files"] if conf["max_acc_files"] >= 0 else 0
for file_name in file_names[start:]:
    with open( file_name ) as f:
        acc_log = os.path.basename( file_name )
        if last_log and acc_log < last_log:
            continue
        log.debug( "importing metrics from: %s" % file_name )
        for line in f:
            tag_new = md5sum( line )
            if not tag or tag_found:
                metric = parse_torque_accounting_log( line )
                if metric:
                    try:
                        store.addMetric( metric )
                        log.debug( "stored: %s" % str( metric ) )
                    except Exception as e:
                        log.error( "failed to handle backlog metric: %s" % str( e.__class__.__name__ ) )
                    update_job_tasks( metric, [cache] )
            else:
                if tag_new == tag:
                    tag_found = True
if tag_new:
    put_tag( tag_file, tag_new, acc_log )
    tag_new = None
if tag and not tag_found:
    log.warn( "tag (%s) not found in any considered acc file (%s)!" % (tag, conf["acc_file_dir"]) )

# push cache that contains currently running jobs collected from backlog to tagger and mongodb's joblist collection
joblist = JobList( conf )
tagger = Tagger( conf )
for metric in cache.getList():
    log.debug( "running job from backlog: %s" % str( metric ))
    update_job_tasks( metric, [tagger, joblist] )
del cache

# tail current accounting file
while True:
    pid = torque_pid()
    file_names = sorted( glob.glob( conf["acc_file_dir"] + "/[0-9]*" ) )
    if not file_names:
        log.warn( "no accounting file found in %s, waiting..." % conf["acc_file_dir"] )
        time.sleep( 30 )
        continue
    file_name = file_names[-1]
    log.info( "monitoring acc file: %s" % file_name )
    for line in tail( pid, file_name ):
        acc_log = os.path.basename( file_name )   # placed here due to sudden SIGTERM
        tag_new = md5sum( line )
        metric = parse_torque_accounting_log( line )
        if metric:
            try:
                store.addMetric( metric )
                log.debug( "saved: %s" % str( metric ) )
            except Exception as e:
                log.error( "failed to handle metric: %s" % str( e ) )
            update_job_tasks( metric, [tagger, joblist] )
